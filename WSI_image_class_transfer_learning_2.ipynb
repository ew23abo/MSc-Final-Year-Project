{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hijjILxrrNwq"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Activation, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "from keras import backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "7TlmmX0VrVDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Loading our data**"
      ],
      "metadata": {
        "id": "3aDwJ82frc7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PatchCamelyon dataset with train, validation, and test splits\n",
        "(train, validation, test), info = tfds.load(\n",
        "    'patch_camelyon',\n",
        "    split=['train', 'validation', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "# Print dataset info\n",
        "print(info)\n",
        "\n",
        "# Reduce dataset size to prevent memory overload\n",
        "train = train.take(10000)  # Limit dataset to 10,000 samples for training\n",
        "validation = validation.take(2000)  # Limit validation set to 2,000 samples\n",
        "test = test.take(2000)  # Limit test set to 2,000 samples\n",
        "\n",
        "# Transform dataset to batches to prevent memory overload\n",
        "batch_size = 64  # Reduce batch size for lower memory usage\n",
        "train = train.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation = validation.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test = test.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Print dataset structure\n",
        "print(train)"
      ],
      "metadata": {
        "id": "2ype4EDhrVF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Visualising our data**\n",
        "\n",
        "Let's take a closer look at the actual data we have just loaded."
      ],
      "metadata": {
        "id": "AVaGSqqcrla2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZDkSvP0rVNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0waXVs2frVPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}